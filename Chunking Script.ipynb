{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull in OS for directory handling and WRDS for data requests\n",
    "## We use 'pprint' to look at outputs, which is a pretty print function for dictionaries\n",
    "\n",
    "import os\n",
    "import wrds\n",
    "import pprint\n",
    "import sqlite3\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### Initialize the connection. Unless you have SSH setup for wrds you will be prompted for a username and password each time you run this. As PUTTY is annoying and this task doesn't take long for most uses, just enter it each time.\n",
    "\n",
    "##\n",
    "\n",
    "# After entering the username and password, it might sit to authenticate for awhile.\n",
    "# This is because it is sending me a 2-Factor Authentication request. This happens one a month.\n",
    "# So just text me before you run this and I will hit the approve button on the 2FA app. \n",
    "\n",
    "db = wrds.Connection()\n",
    "\n",
    "## When prompted to create a pgpass file, just type \"n\". As we are not using PUTTY, this will do nothing, not to worry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Library and Table\n",
    "library = 'ciq_transcripts'\n",
    "table = \"ciqtranscriptcomponent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aha_sample',\n",
       " 'ahasamp',\n",
       " 'auditsmp',\n",
       " 'auditsmp_all',\n",
       " 'bank',\n",
       " 'bank_all',\n",
       " 'bank_premium_samp',\n",
       " 'banksamp',\n",
       " 'block',\n",
       " 'block_all',\n",
       " 'boardex_trial',\n",
       " 'boardsmp',\n",
       " 'bvd_amadeus_trial',\n",
       " 'bvd_bvdbankf_trial',\n",
       " 'bvd_orbis_trial',\n",
       " 'bvdsamp',\n",
       " 'calcbench_trial',\n",
       " 'calcbnch',\n",
       " 'cboe',\n",
       " 'cboe_all',\n",
       " 'cboe_sample',\n",
       " 'cboesamp',\n",
       " 'ciq',\n",
       " 'ciq_common',\n",
       " 'ciq_transcripts',\n",
       " 'ciqsamp',\n",
       " 'ciqsamp_capstrct',\n",
       " 'ciqsamp_common',\n",
       " 'ciqsamp_keydev',\n",
       " 'ciqsamp_pplintel',\n",
       " 'ciqsamp_ratings',\n",
       " 'ciqsamp_transactions',\n",
       " 'ciqsamp_transcripts',\n",
       " 'cisdmsmp',\n",
       " 'columnar',\n",
       " 'comp',\n",
       " 'comp_bank',\n",
       " 'comp_bank_daily',\n",
       " 'comp_global',\n",
       " 'comp_global_daily',\n",
       " 'comp_na_annual_all',\n",
       " 'comp_na_daily_all',\n",
       " 'comp_na_monthly_all',\n",
       " 'comp_segments_hist',\n",
       " 'comp_segments_hist_daily',\n",
       " 'compa',\n",
       " 'compb',\n",
       " 'compg',\n",
       " 'compm',\n",
       " 'compsamp',\n",
       " 'compsamp_all',\n",
       " 'compsamp_snapshot',\n",
       " 'compseg',\n",
       " 'contrib',\n",
       " 'contrib_as_filed_financials',\n",
       " 'contrib_char_returns',\n",
       " 'contrib_corporate_culture',\n",
       " 'contrib_general',\n",
       " 'contrib_global_factor',\n",
       " 'contrib_intangible_value',\n",
       " 'contrib_kpss',\n",
       " 'contrib_liva',\n",
       " 'crsp',\n",
       " 'crsp_a_ccm',\n",
       " 'crsp_a_stock',\n",
       " 'crspsamp',\n",
       " 'crspsamp_all',\n",
       " 'crspsamp_mf',\n",
       " 'csmsamp_all',\n",
       " 'dealscan',\n",
       " 'djones',\n",
       " 'djones_all',\n",
       " 'dmef',\n",
       " 'dmef_all',\n",
       " 'doe',\n",
       " 'doe_all',\n",
       " 'etfg_samp',\n",
       " 'etfgsamp',\n",
       " 'factsamp_all',\n",
       " 'factsamp_revere',\n",
       " 'ff',\n",
       " 'ff_all',\n",
       " 'fisdsamp',\n",
       " 'fisdsamp_all',\n",
       " 'fjc',\n",
       " 'fjc_linking',\n",
       " 'fjc_litigation',\n",
       " 'frb',\n",
       " 'frb_all',\n",
       " 'fssamp',\n",
       " 'ftsesamp',\n",
       " 'ftsesamp_russell_us',\n",
       " 'gutenberg',\n",
       " 'hfrsamp',\n",
       " 'hfrsamp_hfrdb',\n",
       " 'ibessamp_kpi',\n",
       " 'ifgrsamp',\n",
       " 'ims_obp_trial',\n",
       " 'imssamp',\n",
       " 'infogroupsamp_business',\n",
       " 'infogroupsamp_residential',\n",
       " 'insdsamp',\n",
       " 'iri',\n",
       " 'iri_all',\n",
       " 'kpisamp',\n",
       " 'macrofin',\n",
       " 'macrofin_comm_trade',\n",
       " 'midas',\n",
       " 'morningstarsamp_cisdm',\n",
       " 'mrktsamp',\n",
       " 'mrktsamp_cds',\n",
       " 'mrktsamp_cdx',\n",
       " 'mrktsamp_msf',\n",
       " 'msci_esg_samp',\n",
       " 'msciesmp',\n",
       " 'msrb',\n",
       " 'msrb_all',\n",
       " 'msrbsamp',\n",
       " 'msrbsamp_all',\n",
       " 'omtrial',\n",
       " 'optionmsamp_europe',\n",
       " 'optionmsamp_us',\n",
       " 'otc',\n",
       " 'otc_endofday',\n",
       " 'phlx',\n",
       " 'phlx_all',\n",
       " 'pitchsmp',\n",
       " 'preqsamp',\n",
       " 'preqsamp_all',\n",
       " 'public',\n",
       " 'public_all',\n",
       " 'pwt',\n",
       " 'pwt_all',\n",
       " 'ravenpack_trial',\n",
       " 'reprisk_sample',\n",
       " 'repsamp',\n",
       " 'revelio_samp',\n",
       " 'revsamp',\n",
       " 'risksamp',\n",
       " 'risksamp_all',\n",
       " 'rpnasamp',\n",
       " 'rq_all',\n",
       " 'rstat_samp',\n",
       " 'rstatsmp',\n",
       " 'sdcsamp',\n",
       " 'secsamp',\n",
       " 'secsamp_all',\n",
       " 'snapsamp',\n",
       " 'snlsamp',\n",
       " 'snlsamp_fig',\n",
       " 'sustain',\n",
       " 'sustainalytics_all',\n",
       " 'sustainalyticssamp_all',\n",
       " 'sustsamp',\n",
       " 'taqmsamp',\n",
       " 'taqmsamp_all',\n",
       " 'taqsamp',\n",
       " 'taqsamp_all',\n",
       " 'totalq',\n",
       " 'totalq_all',\n",
       " 'tr_common',\n",
       " 'tr_dealscan',\n",
       " 'tr_sdc_samples',\n",
       " 'trace',\n",
       " 'trace_enhanced',\n",
       " 'trace_standard',\n",
       " 'trcommon',\n",
       " 'trcstsmp',\n",
       " 'trdbdmismp',\n",
       " 'trdbwbsmp',\n",
       " 'trdssamp',\n",
       " 'tresgsmp',\n",
       " 'trsamp',\n",
       " 'trsamp_all',\n",
       " 'trsamp_db_dmi',\n",
       " 'trsamp_db_wb',\n",
       " 'trsamp_ds_eq',\n",
       " 'trsamp_dscom',\n",
       " 'trsamp_dsecon',\n",
       " 'trsamp_dsfut',\n",
       " 'trsamp_esg',\n",
       " 'trsamp_sdc_ma',\n",
       " 'trsamp_sdc_ni',\n",
       " 'trsamp_worldscope',\n",
       " 'trucost_samp',\n",
       " 'twoiq_samp',\n",
       " 'twoiqsmp',\n",
       " 'wappsamp',\n",
       " 'wenvsmp',\n",
       " 'wmfsmp',\n",
       " 'wrds_environmental_samp',\n",
       " 'wrds_insiders_samp',\n",
       " 'wrds_lib_internal',\n",
       " 'wrds_mutualfund_samp',\n",
       " 'wrdsapps',\n",
       " 'wrdsapps_eushort',\n",
       " 'wrdsapps_evtstudy_int',\n",
       " 'wrdsapps_evtstudy_lr',\n",
       " 'wrdsapps_evtstudy_us',\n",
       " 'wrdsapps_finratio',\n",
       " 'wrdsapps_finratio_ccm',\n",
       " 'wrdsapps_link_comp_eushort',\n",
       " 'wrdsapps_link_crsp_bond',\n",
       " 'wrdsapps_link_crsp_factset',\n",
       " 'wrdsapps_link_crsp_taq',\n",
       " 'wrdsapps_link_supplychain',\n",
       " 'wrdsapps_patents',\n",
       " 'wrdsapps_subsidiary',\n",
       " 'wrdsapps_windices',\n",
       " 'wrdsappssamp_all',\n",
       " 'wrdssec_midas',\n",
       " 'zacksamp',\n",
       " 'zacksamp_all']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### List out the libraries SMU subscribes to:\n",
    "\n",
    "# We are looking for transcripts provided by Capital IQ, so we will use ciq_transcripts\n",
    "\n",
    "db.list_libraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_available_from_library = db.list_tables(library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ciqtranscript',\n",
      " 'ciqtranscriptcollectiontype',\n",
      " 'ciqtranscriptcomponent',\n",
      " 'ciqtranscriptcomponenttype',\n",
      " 'ciqtranscriptdelayreason',\n",
      " 'ciqtranscriptdelayreasontype',\n",
      " 'ciqtranscriptperson',\n",
      " 'ciqtranscriptpresentationtype',\n",
      " 'ciqtranscriptspeakertype',\n",
      " 'wrds_transcript_detail',\n",
      " 'wrds_transcript_person']\n"
     ]
    }
   ],
   "source": [
    "### Run the following to print the tables\n",
    "\n",
    "pprint.pprint(tables_available_from_library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The following loop will print all the tables available in the pre-pecified library, and their number of rows. \n",
    "\n",
    "for table in tables_available_from_library:\n",
    "    row_count = db.get_row_count(library=library,table=table)\n",
    "    print(f\"{table} has {row_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To see what the various tables look like, run the following, then inspect them with the Jupyter variable viewer in VS Code. \n",
    "\n",
    "\n",
    "for table in tables_available_from_library:\n",
    "    globals()[table] = db.get_table(library=library,table=table,obs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If the tables are not huge, you can run the following. However, if they more than a couple million observations each you will need a lot of RAM\n",
    "### For very large tables, see the next cell\n",
    "\n",
    "output_directory = 'D:\\Data\\Sustainalytics'  ### Put your output directory here - I recommend naming it something similar to the WRDS library name for later referencing. \n",
    "os.chdir(output_directory)\n",
    "\n",
    "for table in tables_available_from_library:\n",
    "    print(f\"Downloading {table}\")\n",
    "    downloaded_table = db.get_table(library=library,table=table)\n",
    "    print(f\"Finished downloading {table}, saving to csv...\")\n",
    "    downloaded_table.to_csv(f\"{table}.csv\",header=True,index=False)\n",
    "    print(f\"Successfully saved {table} to CSV. Moving to next table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will simply tell you the directory you are working in so you can find the files you just downloaded.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from wrds import Connection\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to establish a WRDS connection\n",
    "def connect_to_wrds():\n",
    "    try:\n",
    "        return Connection()\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to WRDS: {e}\")\n",
    "        time.sleep(10)  # Retry after 10 seconds\n",
    "        return connect_to_wrds()\n",
    "\n",
    "# Specify your output directory\n",
    "output_directory = 'D:/wrdsTables/ciqtranscriptcomponent_chunks'\n",
    "os.makedirs(output_directory, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# Define library and table\n",
    "library = 'ciq_transcripts'  # Replace with the correct library name\n",
    "table = 'ciqtranscriptcomponent'  # Replace with the correct table name\n",
    "\n",
    "# WRDS connection\n",
    "global db\n",
    "db = connect_to_wrds()\n",
    "\n",
    "# Fetch distinct `transcriptid` values, excluding NULLs\n",
    "def fetch_transcript_ids():\n",
    "    print(\"Fetching unique transcript IDs...\")\n",
    "    transcript_ids_query = f\"\"\"\n",
    "        SELECT DISTINCT transcriptid\n",
    "        FROM {library}.{table}\n",
    "        WHERE transcriptid IS NOT NULL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        transcript_ids_result = db.raw_sql(transcript_ids_query)\n",
    "        transcript_ids = [row.transcriptid for row in transcript_ids_result.itertuples(index=False) if row.transcriptid is not None]\n",
    "        print(f\"Found {len(transcript_ids)} unique transcript IDs.\")\n",
    "        return transcript_ids  # Return all transcript IDs\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript IDs: {e}\")\n",
    "        return []\n",
    "\n",
    "# Save data chunk to CSV\n",
    "def save_to_csv(chunk, output_file):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write column headers\n",
    "        writer.writerow(chunk.columns)\n",
    "        # Write rows of data\n",
    "        writer.writerows(chunk.values)\n",
    "\n",
    "# Process a single transcript ID\n",
    "def process_transcript(transcript_id, index, total_ids):\n",
    "    global db\n",
    "    output_file = os.path.join(output_directory, f\"{int(transcript_id)}.csv\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"[{index}/{total_ids}] File {output_file} already exists. Skipping transcriptid {transcript_id}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{index}/{total_ids}] Downloading data for transcriptid {transcript_id} from table: {table}\")\n",
    "\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            # Construct SQL query to fetch data for the specific transcript ID\n",
    "            sql_query = f\"\"\"\n",
    "                SELECT *\n",
    "                FROM {library}.{table}\n",
    "                WHERE transcriptid = {int(transcript_id)}\n",
    "                ORDER BY componentorder ASC\n",
    "            \"\"\"\n",
    "\n",
    "            # Fetch data\n",
    "            chunk = db.raw_sql(sql_query)\n",
    "\n",
    "            # Check if data is empty\n",
    "            if chunk.empty:\n",
    "                print(f\"[{index}/{total_ids}] No data found for transcriptid {transcript_id}. Skipping...\")\n",
    "                return\n",
    "\n",
    "            # Save the data to a CSV file\n",
    "            save_to_csv(chunk, output_file)\n",
    "            print(f\"[{index}/{total_ids}] Successfully saved data for transcriptid {transcript_id} to {output_file}.\")\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{index}/{total_ids}] Error processing transcriptid {transcript_id}: {e}\")\n",
    "            retries -= 1\n",
    "            print(f\"[{index}/{total_ids}] Retrying... {retries} attempts left.\")\n",
    "            if retries == 0:\n",
    "                print(f\"[{index}/{total_ids}] Failed to process transcriptid {transcript_id} after multiple attempts. Skipping...\")\n",
    "            # Attempt to reconnect to the database if the connection was lost\n",
    "            db = connect_to_wrds()\n",
    "\n",
    "# Main loop for downloading data with threading\n",
    "def download_transcripts_parallel(transcript_ids):\n",
    "    total_ids = len(transcript_ids)\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(\n",
    "            lambda args: process_transcript(*args),\n",
    "            [(transcript_id, i + 1, total_ids) for i, transcript_id in enumerate(transcript_ids)],\n",
    "        )\n",
    "\n",
    "# Fetch and process transcript data\n",
    "transcript_ids = fetch_transcript_ids()\n",
    "download_transcripts_parallel(transcript_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If a cell errors out, and when you try to rerun the query, you get a \"cannot proceed until last invalid transaction is rolled back\"\n",
    "### Or similar error, run this cell.\n",
    "\n",
    "transactions_roller = db.connection.rollback()\n",
    "transactions_roller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to delete a large folder -- irreversible\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"your file path\", ignore_errors=True)  # Replace with the actual folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
